services:
  proxy:
    build: .
    ports:
      - "8081:8081"
    environment:
      BATCH_PROXY__INFERENCE_API__TARGET_URL: "http://inference-api"
      BATCH_PROXY__BATCH__MAX_BATCH_SIZE: 8
      RUST_LOG: "debug"

  inference-api:
    image: "ghcr.io/huggingface/text-embeddings-inference:cpu-1.8"
    ports:
      - "8080:80"
    volumes:
      - "./text-embeddings-data:/data"
    environment:
      MODEL_ID: "nomic-ai/nomic-embed-text-v1.5"

