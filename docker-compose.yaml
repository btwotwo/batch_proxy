services:
  proxy:
    build: .
    ports:
      - "8081:8081"
    environment:
      BATCH_PROXY__INFERENCE_API__TARGET_URL: "http://inference-api"
      BATCH_PROXY__BATCH__MAX_BATCH_SIZE: 8
      RUST_LOG: "debug"

  inference-api:
    image: "ghcr.io/huggingface/text-embeddings-inference:cpu-1.8"
    ports:
      - "8080:80"
    volumes:
      - "./text-embeddings-data:/data"
    environment:
      MODEL_ID: "nomic-ai/nomic-embed-text-v1.5"
      
  inference-api-gpu:
    image: "ghcr.io/huggingface/text-embeddings-inference:turing-1.8"
    ports:
      - "8080:80"
    volumes:
      - "./text-embeddings-data:/data"
    environment:
      MODEL_ID: "nomic-ai/nomic-embed-text-v1.5"
      DTYPE: "float16"
      MAX_BATCH_TOKENS: 8192
    deploy:             
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]

