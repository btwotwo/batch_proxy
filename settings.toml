[api]
target_port = 8081

[inference_api]
target_url = "http://localhost:8080"

[batch]
max_batch_size = 8
max_waiting_time_ms = 500
