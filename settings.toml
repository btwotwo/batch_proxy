[api]
target_port = 8081

[inference_api]
target_url = "http://localhost:8080"

[batch]
max_batch_size = 32
max_waiting_time_ms = 50
